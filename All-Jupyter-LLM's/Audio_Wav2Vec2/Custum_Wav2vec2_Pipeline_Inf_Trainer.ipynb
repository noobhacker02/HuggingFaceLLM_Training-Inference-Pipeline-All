{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81502bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"common_voice\", \"en\", split=\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "def preprocess(batch):\n",
    "    audio = batch[\"audio\"][\"array\"]\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c880757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for sample in dataset.select(range(5)):\n",
    "    input_values = torch.tensor([sample[\"input_values\"]])\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c784320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-ft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions.argmax(-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    # You can use WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f408f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wav2Vec2_env",
   "language": "python",
   "name": "wav2vec2_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
